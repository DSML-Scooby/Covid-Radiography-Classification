{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e640136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddedDllDirectory('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin')>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Add directory for NVIDIA gpu. Ignore if not Windows\n",
    "os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0318e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "# Confirm tensorflow running on gpu\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a06e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992ac7f",
   "metadata": {},
   "source": [
    "### Base CovNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10640390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the conv net base\n",
    "\n",
    "\n",
    "# complete this function\n",
    "def build_base_convnet_model():\n",
    "    \"\"\"Re-create the model from the first prompt, but with a different input shape\"\"\"\n",
    "    \n",
    "    # Return this variable\n",
    "    model = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    inputs = keras.Input(shape = (299, 299, 3))\n",
    "    x = keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')(inputs)\n",
    "    x = keras.layers.MaxPooling2D(pool_size = 2)(x)\n",
    "    x = keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size = 2)(x)\n",
    "    x = keras.layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    outputs = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_image_data(base_path: str) -> tuple:\n",
    "    \"\"\"Write a function that accepts a base path that contains all of the directories, and creates training,\n",
    "    validation and test sets\"\"\"\n",
    "    \n",
    "    # Return these variables from the function\n",
    "    train_data = keras.utils.image_dataset_from_directory(f'{base_path}/train', \n",
    "                                                          image_size = (299, 299),\n",
    "                                                          batch_size = 64)\n",
    "\n",
    "    validation_data = keras.utils.image_dataset_from_directory(f'{base_path}/val', \n",
    "                                                          image_size = (299, 299),\n",
    "                                                          batch_size = 64)\n",
    "    \n",
    "    test_data = keras.utils.image_dataset_from_directory(f'{base_path}/test', \n",
    "                                                          image_size = (299, 299),\n",
    "                                                          batch_size = 64)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "def fit_model(model, train_set, validation_set):\n",
    "    \"\"\"Fit a model with the above stated criteria\"\"\"\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience = 10)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    model.fit(train_set, \n",
    "              validation_data = validation_set, \n",
    "              callbacks = [early_stopping], \n",
    "              epochs = 500)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dee887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8284 files belonging to 2 classes.\n",
      "Found 2761 files belonging to 2 classes.\n",
      "Found 2763 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to create your base convolution model and training, validation & test sets\n",
    "base_mod = build_base_convnet_model()\n",
    "train_data, validation_data, test_data = load_image_data('../data/COVID-19_Radiography_Dataset/TwoClasses/split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f09b370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 66s 449ms/step - loss: 67.5490 - accuracy: 0.7403 - val_loss: 0.3755 - val_accuracy: 0.8356\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 20s 155ms/step - loss: 24.8731 - accuracy: 0.7859 - val_loss: 3.1400 - val_accuracy: 0.7841\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 20s 152ms/step - loss: 3.9616 - accuracy: 0.8419 - val_loss: 0.4280 - val_accuracy: 0.8526\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 22s 169ms/step - loss: 26.9102 - accuracy: 0.8416 - val_loss: 2.7879 - val_accuracy: 0.8700\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 20s 152ms/step - loss: 2.7133 - accuracy: 0.8842 - val_loss: 0.6322 - val_accuracy: 0.8627\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 20s 150ms/step - loss: 3.0722 - accuracy: 0.9019 - val_loss: 0.3973 - val_accuracy: 0.8939\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 21s 159ms/step - loss: 2.6714 - accuracy: 0.9056 - val_loss: 1.4136 - val_accuracy: 0.8939\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 20s 156ms/step - loss: 1.3814 - accuracy: 0.9328 - val_loss: 4.3990 - val_accuracy: 0.7124\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 21s 158ms/step - loss: 3.3792 - accuracy: 0.9169 - val_loss: 1.4519 - val_accuracy: 0.9095\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 22s 165ms/step - loss: 1.3339 - accuracy: 0.9440 - val_loss: 1.4350 - val_accuracy: 0.8982\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 21s 161ms/step - loss: 2.3035 - accuracy: 0.9394 - val_loss: 1.5423 - val_accuracy: 0.9044\n"
     ]
    }
   ],
   "source": [
    "fitted_model = fit_model(base_mod, train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9c9980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 3s 70ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 189,  535],\n",
       "       [ 512, 1527]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test labels from image_dataset\n",
    "test_labels = np.concatenate([y for x, y in test_data], axis=0)\n",
    "\n",
    "# Flatten function needed to flatten nested lists from predictions\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# Some predictions are not correctly labeled as 1,0\n",
    "predictions = np.where(fitted_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f71fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.27      0.26      0.27       724\n",
      "Normal (Class 1)       0.74      0.75      0.74      2039\n",
      "\n",
      "        accuracy                           0.62      2763\n",
      "       macro avg       0.51      0.50      0.50      2763\n",
      "    weighted avg       0.62      0.62      0.62      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e5f16",
   "metadata": {},
   "source": [
    "### Inception V3 Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3f9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bc3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    \n",
    "    # return this variable\n",
    "    model = None\n",
    "\n",
    "    model_input = keras.Input(shape = (299, 299, 3))\n",
    "    \n",
    "    base_model = InceptionV3(input_shape = (299, 299, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # make the weights in the base model non-trainable\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # combine the base model with a dense layer and output layer for the 10 classes\n",
    "    # the preprocess_input transforms input data according to how the model was trained\n",
    "    \n",
    "    x = keras.applications.inception_v3.preprocess_input(model_input)\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(model_input, output)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6e18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_mod = build_transfer_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c0ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 31s 208ms/step - loss: 5.9614 - accuracy: 0.8166 - val_loss: 0.2429 - val_accuracy: 0.9218\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 25s 193ms/step - loss: 0.4760 - accuracy: 0.8859 - val_loss: 0.8134 - val_accuracy: 0.8196\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 25s 193ms/step - loss: 0.3964 - accuracy: 0.9069 - val_loss: 0.1400 - val_accuracy: 0.9522\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.2737 - accuracy: 0.9314 - val_loss: 0.1448 - val_accuracy: 0.9511\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 26s 197ms/step - loss: 0.2157 - accuracy: 0.9394 - val_loss: 0.1240 - val_accuracy: 0.9598\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.1798 - accuracy: 0.9497 - val_loss: 0.1472 - val_accuracy: 0.9602\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.1961 - accuracy: 0.9505 - val_loss: 0.1655 - val_accuracy: 0.9631\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 25s 191ms/step - loss: 0.1267 - accuracy: 0.9617 - val_loss: 0.1652 - val_accuracy: 0.9656\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.1180 - accuracy: 0.9657 - val_loss: 0.1690 - val_accuracy: 0.9540\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.0803 - accuracy: 0.9701 - val_loss: 0.1663 - val_accuracy: 0.9678\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.0860 - accuracy: 0.9715 - val_loss: 0.1982 - val_accuracy: 0.9641\n",
      "Epoch 12/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.1017 - accuracy: 0.9731 - val_loss: 0.1791 - val_accuracy: 0.9627\n",
      "Epoch 13/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.0895 - accuracy: 0.9736 - val_loss: 0.2251 - val_accuracy: 0.9616\n",
      "Epoch 14/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.0682 - accuracy: 0.9761 - val_loss: 0.1583 - val_accuracy: 0.9674\n",
      "Epoch 15/500\n",
      "130/130 [==============================] - 25s 193ms/step - loss: 0.0589 - accuracy: 0.9797 - val_loss: 0.2112 - val_accuracy: 0.9660\n"
     ]
    }
   ],
   "source": [
    "inception_model = fit_model(transfer_learning_mod, train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553fcf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 139ms/step - loss: 0.1545 - accuracy: 0.9678\n"
     ]
    }
   ],
   "source": [
    "inc_results = inception_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2843cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 3s 69ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create test labels from image_dataset\n",
    "test_labels = np.concatenate([y for x, y in test_data], axis=0)\n",
    "\n",
    "# Flatten function needed to flatten nested lists from predictions\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# Some predictions are not correctly labeled as 1,0\n",
    "predictions = np.where(fitted_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "sns.heatmap(cm, annot = True, cmap='Blues', fmt='.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a47e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.25      0.24      0.25       724\n",
      "Normal (Class 1)       0.73      0.74      0.74      2039\n",
      "\n",
      "        accuracy                           0.61      2763\n",
      "       macro avg       0.49      0.49      0.49      2763\n",
      "    weighted avg       0.61      0.61      0.61      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5ae8e",
   "metadata": {},
   "source": [
    "### ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c1e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd2d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    \n",
    "    # return this variable\n",
    "    model = None\n",
    "\n",
    "    model_input = keras.Input(shape = (299, 299, 3))\n",
    "    \n",
    "    base_model = ResNet50(input_shape = (299, 299, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # make the weights in the base model non-trainable\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # combine the base model with a dense layer and output layer for the 10 classes\n",
    "    # the preprocess_input transforms input data according to how the model was trained\n",
    "    \n",
    "    x = keras.applications.resnet50.preprocess_input(model_input)\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(model_input, output)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db51ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_mod = build_transfer_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be995ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 29s 202ms/step - loss: 5.3746 - accuracy: 0.8904 - val_loss: 0.9255 - val_accuracy: 0.8989\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.5181 - accuracy: 0.9357 - val_loss: 0.2654 - val_accuracy: 0.9276\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.2972 - accuracy: 0.9536 - val_loss: 0.0997 - val_accuracy: 0.9739\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 0.3004 - accuracy: 0.9617 - val_loss: 0.1871 - val_accuracy: 0.9750\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.1564 - accuracy: 0.9714 - val_loss: 0.6624 - val_accuracy: 0.9457\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.1222 - accuracy: 0.9785 - val_loss: 0.1375 - val_accuracy: 0.9826\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.0932 - accuracy: 0.9819 - val_loss: 0.1303 - val_accuracy: 0.9852\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 24s 185ms/step - loss: 0.0771 - accuracy: 0.9850 - val_loss: 0.1252 - val_accuracy: 0.9855\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 24s 185ms/step - loss: 0.0806 - accuracy: 0.9867 - val_loss: 0.2024 - val_accuracy: 0.9819\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.0750 - accuracy: 0.9866 - val_loss: 0.1723 - val_accuracy: 0.9848\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 24s 185ms/step - loss: 0.0487 - accuracy: 0.9914 - val_loss: 0.2287 - val_accuracy: 0.9801\n",
      "Epoch 12/500\n",
      "130/130 [==============================] - 24s 185ms/step - loss: 0.0514 - accuracy: 0.9901 - val_loss: 0.2568 - val_accuracy: 0.9819\n",
      "Epoch 13/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.0510 - accuracy: 0.9917 - val_loss: 0.3077 - val_accuracy: 0.9739\n"
     ]
    }
   ],
   "source": [
    "resnet_model = fit_model(transfer_learning_mod, train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d10a6a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 130ms/step - loss: 0.1733 - accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "results = resnet_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eafa862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 127ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some predictions are continuous and thus not labeled as 1,0\n",
    "predictions = np.where(resnet_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "sns.heatmap(cm, annot = True, cmap='Blues', fmt='.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d72d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.25      0.27      0.26       724\n",
      "Normal (Class 1)       0.73      0.71      0.72      2039\n",
      "\n",
      "        accuracy                           0.60      2763\n",
      "       macro avg       0.49      0.49      0.49      2763\n",
      "    weighted avg       0.61      0.60      0.60      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cc111",
   "metadata": {},
   "source": [
    "### ResNet with Dropout 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "449664fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    \n",
    "    # return this variable\n",
    "    model = None\n",
    "\n",
    "    model_input = keras.Input(shape = (299, 299, 3))\n",
    "    \n",
    "    base_model = ResNet50(input_shape = (299, 299, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # make the weights in the base model non-trainable\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # combine the base model with a dense layer and output layer for the 10 classes\n",
    "    # the preprocess_input transforms input data according to how the model was trained\n",
    "    \n",
    "    x = keras.applications.resnet50.preprocess_input(model_input)\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    output = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(model_input, output)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e92d1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_mod = build_transfer_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efa5400e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 27s 191ms/step - loss: 7.9719 - accuracy: 0.8921 - val_loss: 0.1208 - val_accuracy: 0.9526\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.3612 - accuracy: 0.9423 - val_loss: 0.1623 - val_accuracy: 0.9736\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 25s 192ms/step - loss: 0.2030 - accuracy: 0.9683 - val_loss: 0.2385 - val_accuracy: 0.9656\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 25s 193ms/step - loss: 0.1247 - accuracy: 0.9769 - val_loss: 0.4654 - val_accuracy: 0.9518\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 25s 190ms/step - loss: 0.0713 - accuracy: 0.9872 - val_loss: 0.3114 - val_accuracy: 0.9717\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.0838 - accuracy: 0.9876 - val_loss: 0.1757 - val_accuracy: 0.9804\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.0663 - accuracy: 0.9896 - val_loss: 0.2059 - val_accuracy: 0.9812\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 24s 185ms/step - loss: 0.0736 - accuracy: 0.9917 - val_loss: 0.1625 - val_accuracy: 0.9837\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 25s 188ms/step - loss: 0.0412 - accuracy: 0.9930 - val_loss: 0.1425 - val_accuracy: 0.9823\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 24s 186ms/step - loss: 0.0562 - accuracy: 0.9940 - val_loss: 0.1749 - val_accuracy: 0.9826\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 24s 187ms/step - loss: 0.0248 - accuracy: 0.9960 - val_loss: 0.1714 - val_accuracy: 0.9859\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "resnet25_model = fit_model(transfer_learning_mod, train_data, validation_data)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e36856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 129ms/step - loss: 0.0959 - accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "results = resnet25_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b798d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 128ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some predictions are continuous and thus not labeled as 1,0\n",
    "predictions = np.where(resnet25_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "sns.heatmap(cm, annot = True, cmap='Blues', fmt='.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e30acc97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.26      0.26      0.26       724\n",
      "Normal (Class 1)       0.74      0.74      0.74      2039\n",
      "\n",
      "        accuracy                           0.61      2763\n",
      "       macro avg       0.50      0.50      0.50      2763\n",
      "    weighted avg       0.61      0.61      0.61      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c9198",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dbd8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and validation data\n",
    "fulltrain = train_data.concatenate(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b723de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 26s 147ms/step - loss: 0.1094 - accuracy: 0.9890\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on 80% data\n",
    "final_model = resnet25_model.fit(fulltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5377df29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c59aa106-c312-4808-b630-ef664355ced3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c59aa106-c312-4808-b630-ef664355ced3/assets\n"
     ]
    }
   ],
   "source": [
    "filename = 'final_model.pkl'\n",
    "pkl.dump(resnet25_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba5fa3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fullmodel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fullmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save full model\n",
    "resnet_model.save('fullmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03386976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 299, 299, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 299, 299, 3)      0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 10, 10, 2048)      23587712  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 204800)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               52429056  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,017,025\n",
      "Trainable params: 52,429,313\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "new_model = tf.keras.models.load_model('fullmodel')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ce3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 7s - loss: 0.1733 - accuracy: 0.9772 - 7s/epoch - 160ms/step\n",
      "Restored model, accuracy: 97.72%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(test_data, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93820d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
