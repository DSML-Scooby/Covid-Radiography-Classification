{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e640136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddedDllDirectory('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin')>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Add directory for NVIDIA gpu. Ignore if not Windows\n",
    "os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0318e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "# Confirm tensorflow running on gpu\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a06e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992ac7f",
   "metadata": {},
   "source": [
    "### Base CovNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10640390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the conv net base\n",
    "\n",
    "\n",
    "# complete this function\n",
    "def build_base_convnet_model():\n",
    "    \"\"\"Re-create the model from the first prompt, but with a different input shape\"\"\"\n",
    "    \n",
    "    # Return this variable\n",
    "    model = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    inputs = keras.Input(shape = (299, 299, 3))\n",
    "    x = keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')(inputs)\n",
    "    x = keras.layers.MaxPooling2D(pool_size = 2)(x)\n",
    "    x = keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size = 2)(x)\n",
    "    x = keras.layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    outputs = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_image_data(base_path: str) -> tuple:\n",
    "    \"\"\"Write a function that accepts a base path that contains all of the directories, and creates training,\n",
    "    validation and test sets\"\"\"\n",
    "    \n",
    "    # Return these variables from the function\n",
    "    train_data = keras.utils.image_dataset_from_directory(f'{base_path}/train', \n",
    "                                                          image_size = (299, 299),\n",
    "                                                          batch_size = 64)\n",
    "\n",
    "    validation_data = keras.utils.image_dataset_from_directory(f'{base_path}/val', \n",
    "                                                          image_size = (299, 299),\n",
    "                                                          batch_size = 64)\n",
    "    \n",
    "    test_data = keras.utils.image_dataset_from_directory(f'{base_path}/test', \n",
    "                                                          image_size = (299, 299),\n",
    "                                                          batch_size = 64)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "def fit_model(model, train_set, validation_set):\n",
    "    \"\"\"Fit a model with the above stated criteria\"\"\"\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience = 10)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    model.fit(train_set, \n",
    "              validation_data = validation_set, \n",
    "              callbacks = [early_stopping], \n",
    "              epochs = 500)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dee887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8284 files belonging to 2 classes.\n",
      "Found 2761 files belonging to 2 classes.\n",
      "Found 2763 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to create your base convolution model and training, validation & test sets\n",
    "base_mod = build_base_convnet_model()\n",
    "train_data, validation_data, test_data = load_image_data('../data/COVID-19_Radiography_Dataset/TwoClasses/split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f09b370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 20s 152ms/step - loss: 91.7088 - accuracy: 0.7443 - val_loss: 0.3974 - val_accuracy: 0.8464\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 20s 149ms/step - loss: 16.3133 - accuracy: 0.7869 - val_loss: 1.2468 - val_accuracy: 0.8059\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 19s 147ms/step - loss: 4.1481 - accuracy: 0.8455 - val_loss: 1.2126 - val_accuracy: 0.8707\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 19s 148ms/step - loss: 3.8081 - accuracy: 0.8741 - val_loss: 0.6180 - val_accuracy: 0.8917\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 19s 147ms/step - loss: 1.2082 - accuracy: 0.8984 - val_loss: 0.4014 - val_accuracy: 0.8997\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 19s 147ms/step - loss: 3.6094 - accuracy: 0.9056 - val_loss: 2.5457 - val_accuracy: 0.8787\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 20s 149ms/step - loss: 14.4908 - accuracy: 0.8903 - val_loss: 9.4728 - val_accuracy: 0.8798\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 19s 148ms/step - loss: 2.3969 - accuracy: 0.9278 - val_loss: 1.9155 - val_accuracy: 0.8953\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 19s 148ms/step - loss: 4.5203 - accuracy: 0.9223 - val_loss: 6.2462 - val_accuracy: 0.8280\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 19s 147ms/step - loss: 1.5243 - accuracy: 0.9380 - val_loss: 3.2797 - val_accuracy: 0.9098\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 19s 147ms/step - loss: 0.5374 - accuracy: 0.9587 - val_loss: 1.6047 - val_accuracy: 0.9142\n"
     ]
    }
   ],
   "source": [
    "fitted_model = fit_model(base_mod, train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a9c9980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 3s 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 197,  527],\n",
       "       [ 482, 1557]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test labels from image_dataset\n",
    "test_labels = np.concatenate([y for x, y in test_data], axis=0)\n",
    "\n",
    "# Flatten function needed to flatten nested lists from predictions\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# Some predictions are not correctly labeled as 1,0\n",
    "predictions = np.where(fitted_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f71fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.29      0.27      0.28       724\n",
      "Normal (Class 1)       0.75      0.76      0.76      2039\n",
      "\n",
      "        accuracy                           0.63      2763\n",
      "       macro avg       0.52      0.52      0.52      2763\n",
      "    weighted avg       0.63      0.63      0.63      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e5f16",
   "metadata": {},
   "source": [
    "### Inception V3 Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f3f9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2bc3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    \n",
    "    # return this variable\n",
    "    model = None\n",
    "\n",
    "    model_input = keras.Input(shape = (299, 299, 3))\n",
    "    \n",
    "    base_model = InceptionV3(input_shape = (299, 299, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # make the weights in the base model non-trainable\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # combine the base model with a dense layer and output layer for the 10 classes\n",
    "    # the preprocess_input transforms input data according to how the model was trained\n",
    "    \n",
    "    x = keras.applications.inception_v3.preprocess_input(model_input)\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(model_input, output)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f6e18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_mod = build_transfer_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4c0ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 29s 202ms/step - loss: 6.5874 - accuracy: 0.8127 - val_loss: 0.2728 - val_accuracy: 0.8823\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.4908 - accuracy: 0.8764 - val_loss: 0.1706 - val_accuracy: 0.9355\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.3683 - accuracy: 0.9034 - val_loss: 0.1333 - val_accuracy: 0.9489\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.3315 - accuracy: 0.9144 - val_loss: 0.1461 - val_accuracy: 0.9576\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.2113 - accuracy: 0.9334 - val_loss: 0.1599 - val_accuracy: 0.9580\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.2455 - accuracy: 0.9355 - val_loss: 0.1445 - val_accuracy: 0.9558\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.1814 - accuracy: 0.9452 - val_loss: 0.2065 - val_accuracy: 0.9587\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.1335 - accuracy: 0.9553 - val_loss: 0.1305 - val_accuracy: 0.9660\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.1349 - accuracy: 0.9590 - val_loss: 0.2184 - val_accuracy: 0.9497\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.1113 - accuracy: 0.9651 - val_loss: 0.2141 - val_accuracy: 0.9504\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.0937 - accuracy: 0.9711 - val_loss: 0.1603 - val_accuracy: 0.9678\n",
      "Epoch 12/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.0887 - accuracy: 0.9710 - val_loss: 0.1716 - val_accuracy: 0.9681\n",
      "Epoch 13/500\n",
      "130/130 [==============================] - 25s 194ms/step - loss: 0.1512 - accuracy: 0.9691 - val_loss: 0.2047 - val_accuracy: 0.9667\n",
      "Epoch 14/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.0744 - accuracy: 0.9754 - val_loss: 0.1723 - val_accuracy: 0.9681\n",
      "Epoch 15/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.0802 - accuracy: 0.9757 - val_loss: 0.1624 - val_accuracy: 0.9714\n",
      "Epoch 16/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.0696 - accuracy: 0.9749 - val_loss: 0.3536 - val_accuracy: 0.9348\n",
      "Epoch 17/500\n",
      "130/130 [==============================] - 25s 195ms/step - loss: 0.0600 - accuracy: 0.9792 - val_loss: 0.2401 - val_accuracy: 0.9547\n",
      "Epoch 18/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.0620 - accuracy: 0.9800 - val_loss: 0.1839 - val_accuracy: 0.9732\n"
     ]
    }
   ],
   "source": [
    "inception_model = fit_model(transfer_learning_mod, train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "553fcf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 138ms/step - loss: 0.1314 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "inc_results = inception_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2843cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 3s 63ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create test labels from image_dataset\n",
    "test_labels = np.concatenate([y for x, y in test_data], axis=0)\n",
    "\n",
    "# Flatten function needed to flatten nested lists from predictions\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# Some predictions are not correctly labeled as 1,0\n",
    "predictions = np.where(fitted_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "sns.heatmap(cm, annot = True, cmap='Blues', fmt='.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a47e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.25      0.23      0.24       724\n",
      "Normal (Class 1)       0.73      0.75      0.74      2039\n",
      "\n",
      "        accuracy                           0.62      2763\n",
      "       macro avg       0.49      0.49      0.49      2763\n",
      "    weighted avg       0.61      0.62      0.61      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5ae8e",
   "metadata": {},
   "source": [
    "### ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9c1e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cd2d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    \n",
    "    # return this variable\n",
    "    model = None\n",
    "\n",
    "    model_input = keras.Input(shape = (299, 299, 3))\n",
    "    \n",
    "    base_model = ResNet50(input_shape = (299, 299, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # make the weights in the base model non-trainable\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # combine the base model with a dense layer and output layer for the 10 classes\n",
    "    # the preprocess_input transforms input data according to how the model was trained\n",
    "    \n",
    "    x = keras.applications.resnet50.preprocess_input(model_input)\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(model_input, output)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0db51ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_mod = build_transfer_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be995ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 29s 205ms/step - loss: 8.1399 - accuracy: 0.8691 - val_loss: 0.1339 - val_accuracy: 0.9674\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.5009 - accuracy: 0.9363 - val_loss: 0.1119 - val_accuracy: 0.9725\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.2827 - accuracy: 0.9575 - val_loss: 0.0957 - val_accuracy: 0.9757\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.1444 - accuracy: 0.9693 - val_loss: 0.0959 - val_accuracy: 0.9768\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.1688 - accuracy: 0.9726 - val_loss: 0.1429 - val_accuracy: 0.9783\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.1540 - accuracy: 0.9763 - val_loss: 0.1662 - val_accuracy: 0.9790\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 26s 195ms/step - loss: 0.0843 - accuracy: 0.9837 - val_loss: 0.1388 - val_accuracy: 0.9804\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.0645 - accuracy: 0.9855 - val_loss: 0.1370 - val_accuracy: 0.9819\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.0621 - accuracy: 0.9871 - val_loss: 0.2431 - val_accuracy: 0.9696\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.0590 - accuracy: 0.9884 - val_loss: 0.1970 - val_accuracy: 0.9804\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.0472 - accuracy: 0.9902 - val_loss: 0.1574 - val_accuracy: 0.9830\n",
      "Epoch 12/500\n",
      "130/130 [==============================] - 26s 196ms/step - loss: 0.0353 - accuracy: 0.9920 - val_loss: 0.1874 - val_accuracy: 0.9837\n",
      "Epoch 13/500\n",
      "130/130 [==============================] - 26s 197ms/step - loss: 0.0346 - accuracy: 0.9934 - val_loss: 0.2087 - val_accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "resnet_model = fit_model(transfer_learning_mod, train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d10a6a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 138ms/step - loss: 0.1247 - accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "results = resnet_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eafa862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 7s 137ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some predictions are continuous and thus not labeled as 1,0\n",
    "predictions = np.where(resnet_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "sns.heatmap(cm, annot = True, cmap='Blues', fmt='.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81d72d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.28      0.27      0.28       724\n",
      "Normal (Class 1)       0.74      0.75      0.74      2039\n",
      "\n",
      "        accuracy                           0.62      2763\n",
      "       macro avg       0.51      0.51      0.51      2763\n",
      "    weighted avg       0.62      0.62      0.62      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cc111",
   "metadata": {},
   "source": [
    "### ResNet with Dropout 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "449664fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    \n",
    "    # return this variable\n",
    "    model = None\n",
    "\n",
    "    model_input = keras.Input(shape = (299, 299, 3))\n",
    "    \n",
    "    base_model = ResNet50(input_shape = (299, 299, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # make the weights in the base model non-trainable\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # combine the base model with a dense layer and output layer for the 10 classes\n",
    "    # the preprocess_input transforms input data according to how the model was trained\n",
    "    \n",
    "    x = keras.applications.resnet50.preprocess_input(model_input)\n",
    "    x = base_model(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    output = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(model_input, output)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e92d1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_mod = build_transfer_learning_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efa5400e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "130/130 [==============================] - 29s 202ms/step - loss: 7.9058 - accuracy: 0.8892 - val_loss: 0.1123 - val_accuracy: 0.9638\n",
      "Epoch 2/500\n",
      "130/130 [==============================] - 26s 202ms/step - loss: 0.5234 - accuracy: 0.9415 - val_loss: 0.0983 - val_accuracy: 0.9725\n",
      "Epoch 3/500\n",
      "130/130 [==============================] - 27s 204ms/step - loss: 0.1808 - accuracy: 0.9685 - val_loss: 0.1128 - val_accuracy: 0.9761\n",
      "Epoch 4/500\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 0.1529 - accuracy: 0.9746 - val_loss: 0.1087 - val_accuracy: 0.9804\n",
      "Epoch 5/500\n",
      "130/130 [==============================] - 26s 201ms/step - loss: 0.0593 - accuracy: 0.9859 - val_loss: 0.0987 - val_accuracy: 0.9779\n",
      "Epoch 6/500\n",
      "130/130 [==============================] - 27s 204ms/step - loss: 0.0421 - accuracy: 0.9891 - val_loss: 0.2067 - val_accuracy: 0.9696\n",
      "Epoch 7/500\n",
      "130/130 [==============================] - 27s 204ms/step - loss: 0.0775 - accuracy: 0.9877 - val_loss: 0.5165 - val_accuracy: 0.9258\n",
      "Epoch 8/500\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 0.0544 - accuracy: 0.9917 - val_loss: 0.1221 - val_accuracy: 0.9866\n",
      "Epoch 9/500\n",
      "130/130 [==============================] - 26s 199ms/step - loss: 0.0211 - accuracy: 0.9941 - val_loss: 0.2131 - val_accuracy: 0.9804\n",
      "Epoch 10/500\n",
      "130/130 [==============================] - 26s 198ms/step - loss: 0.0368 - accuracy: 0.9929 - val_loss: 0.2775 - val_accuracy: 0.9812\n",
      "Epoch 11/500\n",
      "130/130 [==============================] - 25s 192ms/step - loss: 0.0248 - accuracy: 0.9951 - val_loss: 0.2017 - val_accuracy: 0.9823\n",
      "Epoch 12/500\n",
      "130/130 [==============================] - 25s 189ms/step - loss: 0.0181 - accuracy: 0.9965 - val_loss: 0.2307 - val_accuracy: 0.9797\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "resnet25_model = fit_model(transfer_learning_mod, train_data, validation_data)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e36856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 131ms/step - loss: 0.1446 - accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "results = resnet25_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22b798d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 6s 133ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some predictions are continuous and thus not labeled as 1,0\n",
    "predictions = np.where(resnet_model.predict(test_data) > 0.5, 1, 0)\n",
    "predictions\n",
    "\n",
    "y_preds = flatten(predictions)\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_preds)\n",
    "sns.heatmap(cm, annot = True, cmap='Blues', fmt='.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e30acc97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      " Covid (Class 0)       0.28      0.28      0.28       724\n",
      "Normal (Class 1)       0.74      0.75      0.75      2039\n",
      "\n",
      "        accuracy                           0.62      2763\n",
      "       macro avg       0.51      0.51      0.51      2763\n",
      "    weighted avg       0.62      0.62      0.62      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Covid (Class 0)', 'Normal (Class 1)']\n",
    "print(classification_report(test_labels, y_preds, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c9198",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dbd8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and validation data\n",
    "fulltrain = train_data.concatenate(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b723de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 30s 172ms/step - loss: 0.1167 - accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on 80% data\n",
    "final_model = resnet_model.fit(fulltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5377df29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://09ebd1af-97e8-4455-b680-1b8136079e68/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://09ebd1af-97e8-4455-b680-1b8136079e68/assets\n"
     ]
    }
   ],
   "source": [
    "filename = 'final_model.pkl'\n",
    "pkl.dump(resnet_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba5fa3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fullmodel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fullmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save full model\n",
    "resnet_model.save('fullmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03386976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 299, 299, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 299, 299, 3)      0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 10, 10, 2048)      23587712  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 204800)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               52429056  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,017,025\n",
      "Trainable params: 52,429,313\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "new_model = tf.keras.models.load_model('fullmodel')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13ce3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 7s - loss: 0.0898 - accuracy: 0.9841 - 7s/epoch - 153ms/step\n",
      "Restored model, accuracy: 98.41%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(test_data, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93820d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
